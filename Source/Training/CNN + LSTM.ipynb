{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YOLOP network and CNN + LSTM are two different NN trained in different time.\n",
    "For the testing or training phase is necessary download the code of YOLOP showed in the readme and used it for preprocessing the data sample for this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da980731",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!export TF_FORCE_GPU_ALLOW_GROWTH='true'\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "sess = InteractiveSession()\n",
    "sess.close()\n",
    "print(tf.test.gpu_device_name())\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    " #!{sys.executable} -m pip install opencv-python\n",
    "import sklearn\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import gc\n",
    "import sklearn.model_selection\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import sklearn.model_selection\n",
    "from pathlib import Path\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import PIL\n",
    "from cv2 import cv2\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Input, Flatten, LSTM, Reshape, TimeDistributed\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, BatchNormalization, TimeDistributed\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM, Input, concatenate, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.layers import GlobalMaxPool2D, ZeroPadding1D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 50\n",
    "H = 50\n",
    "channels = 2\n",
    "\n",
    "nbout=4\n",
    "NF = 24\n",
    "\n",
    "\n",
    "HEIGHT, WIDTH, CHANNELS = (150,150, 4)#(int(736/4), int(1280/4))\n",
    "NBFRAME = NF\n",
    "idx = 30\n",
    "\n",
    "\n",
    "class dataset():\n",
    "    def __init__(self, directory):\n",
    "        \n",
    "        folder = []\n",
    "        ##=============================== GET PATH AND LABEL OF FILES ===============================\n",
    "        for i in directory:\n",
    "            folder.append(Path(i))\n",
    "\n",
    "        # obtain all the filenames of files inside all the class folders \n",
    "        # going through each class folder one at a time\n",
    "        self.fnames, labels = [], []\n",
    "        for i in folder:\n",
    "            for label in sorted(os.listdir(i)):\n",
    "                self.fnames.append(os.path.join(i, label))#, fname))\n",
    "                for fname in os.listdir(os.path.join(i, label)):\n",
    "                    labels.append(label)\n",
    "                \n",
    "        # prepare a mapping between the label names (strings) and indices (ints)\n",
    "        self.label2index = {label:index for index, label in enumerate(sorted(set(labels)))} \n",
    "        \n",
    "        # convert the list of label names into an array of label indices\n",
    "        self.label_array1 = np.array([self.label2index[label] for label in labels], dtype=int)        \n",
    "        self.label_array = []\n",
    "        for i in np.arange(len(self.label_array1)):\n",
    "            self.label_array.append(np.zeros(4))\n",
    "            \n",
    "            self.label_array[i][self.label_array1[i]] = 1\n",
    "        #create_dataset()\n",
    "    \n",
    "        self.data_array1 = []\n",
    "        self.data_array2 = []\n",
    "        self.data_array3 = []\n",
    "        \n",
    "        old = 0\n",
    "    \n",
    "        for i in np.arange(len(self.fnames)):\n",
    "            for folname in os.listdir(self.fnames[i]):\n",
    "                fname = os.path.join(self.fnames[i], folname)\n",
    "                files = [f for f in os.listdir(fname) if os.path.isfile(os.path.join(fname, f))]\n",
    "    \n",
    "                clips1 = []\n",
    "                clips2 = []\n",
    "                clips3 = []\n",
    "                \n",
    "                for f in files:\n",
    "#==========================================load del file f====================================================\n",
    "                    try:\n",
    "                        clip = np.load(os.path.join(fname,f), allow_pickle=True)\n",
    "                        read_wrong = False\n",
    "                    except:\n",
    "                        read_wrong = True\n",
    "                        print(\"error\")\n",
    "\n",
    "                    # extract 3 kind of infomation:\n",
    "                    #     x1 = car bounding box\n",
    "                    #     x2 = road drivable segmentation\n",
    "                    #     x3 = lane linee segmentation\n",
    "                    if(read_wrong == False):\n",
    "                        x1_ = clip[0][0]\n",
    "\n",
    "                        img = clip[0][1]\n",
    "                        \n",
    "                    clips1.append(np.array((x1_)))#.ravel(), clip[0][1].ravel(), clip[0][2].ravel(), clip[0][3].ravel())))\n",
    "                    clips2.append(np.array((img)))\n",
    "                                    \n",
    "                self.data_array1.append(clips1)\n",
    "                self.data_array2.append(clips2)\n",
    "                                                \n",
    "                old=int(folname)\n",
    "                clips1 = []\n",
    "                clips2 = []\n",
    "                \n",
    "        print1 = self.data_array1.copy()\n",
    "        print2 = self.data_array2.copy()\n",
    "        \n",
    "        self.data_array1 = np.zeros((np.shape(self.data_array1)[0], NF, idx, 7, 1))\n",
    "        self.data_array2 = np.zeros((np.shape(self.data_array2)[0], NF, W, H, channels))\n",
    "        \n",
    "        for i in np.arange(np.shape(self.data_array1)[0]):\n",
    "            try:\n",
    "                self.data_array1[i] = print1[i]\n",
    "            except:\n",
    "                print(\"error\")\n",
    "            try:\n",
    "                self.data_array2[i] = print2[i]\n",
    "            except:\n",
    "                print(\"error\")\n",
    "            \n",
    "        self.data_array1 = np.array(self.data_array1)\n",
    "        self.data_array2 = np.array(self.data_array2)\n",
    "        \n",
    "    def getitem(self):\n",
    "        \n",
    "        x1_TR, x1_TS, x2_TR, x2_TS, y_TR, y_TS = sklearn.model_selection.train_test_split(self.data_array1,self.data_array2, self.label_array, test_size=0.25)\n",
    "        \n",
    "        return x1_TR, x1_TS, x2_TR, x2_TS, y_TR, y_TS\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.data_array1)\n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3610546d"
   },
   "source": [
    "# Implement Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def LSTM_NET():\n",
    "\n",
    "    momentum = .9\n",
    "\n",
    "    in1 = Input((idx, 7, 1))\n",
    "\n",
    "    in2 = Input((W, H, 2))\n",
    "\n",
    "    flt1 = Flatten()(in1)\n",
    "\n",
    "    x2 = Conv2D(64, (3,3), input_shape=((W,H,2)),padding='same', activation='relu')(in2)\n",
    "    x2 = BatchNormalization(momentum=momentum)(x2)\n",
    "    x2 = MaxPool2D()(x2)\n",
    "    x2 = Conv2D(128, (3,3), padding='same', activation='relu')(x2)\n",
    "    x2 = BatchNormalization(momentum=momentum)(x2)\n",
    "    x2 = MaxPool2D()(x2)\n",
    "    x2 = Conv2D(256, (3,3), padding='same', activation='relu')(x2)\n",
    "    x2 = BatchNormalization(momentum=momentum)(x2)\n",
    "    x2 = GlobalMaxPool2D()(x2) #vettore in uscita pari a 128\n",
    "    x2 = Dense(128, activation='relu')(x2)\n",
    "    x2 = Dropout(.2)(x2)\n",
    "    x2 = Dense(64, activation='relu')(x2)\n",
    "    x2 = Dropout(.2)(x2)\n",
    "    x2 = Dense(32, activation='relu')(x2)\n",
    "    x2 = Dropout(.2)(x2)\n",
    "    x2 = Dense(25, activation='relu')(x2)\n",
    "    \n",
    "    preModel1 = Model(inputs=in1, outputs = flt1)\n",
    "    preModel2 = Model(inputs=in2, outputs = x2)\n",
    "\n",
    "    in1 = tf.keras.Input(shape=(NF, idx, 7, 1))\n",
    "    in2 = tf.keras.Input(shape=(NF, W, H, 2))\n",
    "    t1 = tf.keras.layers.TimeDistributed(preModel1)(in1)\n",
    "    t2 = tf.keras.layers.TimeDistributed(preModel2)(in2)\n",
    "\n",
    "    x = concatenate([t1, t2])\n",
    "\n",
    "    x = LSTM(int(235*2/3), return_sequences=True)(x)\n",
    "    x = LSTM(int(235*4/9))(x)\n",
    "\n",
    "    #x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(nbout, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[in1, in2], outputs = x)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NF = 80#number of frames used\n",
    "NBFRAME = NF\n",
    "        \n",
    "path = [\"Dataset1\", \"Dataset2\"]\n",
    "\n",
    "#Creating dataset\n",
    "x = dataset(path)\n",
    "x1_TR, x1_TS, x2_TR, x2_TS, y_TR, y_TS = x.getitem()\n",
    "\n",
    "x1_TR = np.asarray(x1_TR)\n",
    "x2_TR = np.asarray(x2_TR)\n",
    "\n",
    "x1_TS = np.asarray(x1_TS)\n",
    "x2_TS = np.asarray(x2_TS)\n",
    "        \n",
    "y_TR = np.asarray(y_TR)\n",
    "y_TS = np.asarray(y_TS)\n",
    "\n",
    "#Creating model\n",
    "model = LSTM_NET()\n",
    "\n",
    "#Launch training\n",
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "        \n",
    "model.compile(\n",
    "    optimizer,\n",
    "    'categorical_crossentropy',\n",
    "    #loss = 'mse',\n",
    "    metrics=['acc']\n",
    ")\n",
    "        \n",
    "EPOCHS=300\n",
    "\n",
    "try:\n",
    "    os.mkdir(str(NF) + ' frames/'+'chkp_'+str(i)+'_'+str(NF))\n",
    "    os.mkdir(str(NF) + ' frames/'+'chkp_'+str(i)+'_'+str(NF)+'/logs')\n",
    "except:\n",
    "    print(\"folder chkp already exist\")\n",
    "        \n",
    "callbacks = [keras.callbacks.ModelCheckpoint(str(NF) + ' frames/'+'chkp_'+str(i)+'_'+str(NF)+'/chkp'+str(NF)+'weights.{epoch:02d}-{val_loss:.2f}.hdf5',verbose=1),keras.callbacks.TensorBoard(log_dir=str(NF) + ' frames/'+'chkp_'+str(i)+'_'+str(NF)+'/'+'/logs')]\n",
    "\n",
    "model.fit(\n",
    "    [x1_TR, x2_TR],\n",
    "    y_TR,\n",
    "    batch_size=12,\n",
    "    validation_data=((x1_TS, x2_TS), y_TS),\n",
    "    verbose=1,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_NN-Copy2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
