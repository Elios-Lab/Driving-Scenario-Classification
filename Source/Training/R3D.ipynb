{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version, sys.platform, sys.executable)\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.utils import _triple\n",
    "\n",
    "from torchsummary import summary\n",
    "from torch import nn, optim\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchviz import make_dot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import *\n",
    "import pylab\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.utils import _triple\n",
    "\n",
    "\n",
    "class SpatioTemporalConv(nn.Module):\n",
    "    r\"\"\"Applies a factored 3D convolution over an input signal composed of several input \n",
    "    planes with distinct spatial and time axes, by performing a 2D convolution over the \n",
    "    spatial axes to an intermediate subspace, followed by a 1D convolution over the time \n",
    "    axis to produce the final output.\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input tensor\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "        kernel_size (int or tuple): Size of the convolving kernel\n",
    "        stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "        padding (int or tuple, optional): Zero-padding added to the sides of the input during their respective convolutions. Default: 0\n",
    "        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n",
    "        super(SpatioTemporalConv, self).__init__()\n",
    "\n",
    "        # if ints are entered, convert them to iterables, 1 -> [1, 1, 1]\n",
    "        kernel_size = _triple(kernel_size)\n",
    "        stride = _triple(stride)\n",
    "        padding = _triple(padding)\n",
    "\n",
    "        # decomposing the parameters into spatial and temporal components by\n",
    "        # masking out the values with the defaults on the axis that\n",
    "        # won't be convolved over. This is necessary to avoid unintentional\n",
    "        # behavior such as padding being added twice\n",
    "        spatial_kernel_size =  [kernel_size[0], kernel_size[1], kernel_size[2]]\n",
    "        spatial_stride =  [stride[0], stride[1], stride[2]]\n",
    "        spatial_padding =  [padding[0], padding[1], padding[2]]\n",
    "\n",
    "        # the spatial conv is effectively a 2D conv due to the \n",
    "        # spatial_kernel_size, followed by batch_norm and ReLU\n",
    "        self.spatial_conv = nn.Conv3d(in_channels, out_channels, spatial_kernel_size,\n",
    "                                    stride=spatial_stride, padding=spatial_padding, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spatial_conv(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalResBlock(nn.Module):\n",
    "    r\"\"\"Single block for the ResNet network. Uses SpatioTemporalConv in \n",
    "        the standard ResNet block layout (conv->batchnorm->ReLU->conv->batchnorm->sum->ReLU)\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of channels in the input tensor.\n",
    "            out_channels (int): Number of channels in the output produced by the block.\n",
    "            kernel_size (int or tuple): Size of the convolving kernels.\n",
    "            downsample (bool, optional): If ``True``, the output size is to be smaller than the input. Default: ``False``\n",
    "        \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, downsample=False):\n",
    "        super(SpatioTemporalResBlock, self).__init__()\n",
    "        \n",
    "        # If downsample == True, the first conv of the layer has stride = 2 \n",
    "        # to halve the residual output size, and the input x is passed \n",
    "        # through a seperate 1x1x1 conv with stride = 2 to also halve it.\n",
    "\n",
    "        # no pooling layers are used inside ResNet\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        # to allow for SAME padding\n",
    "        padding = kernel_size//2\n",
    "\n",
    "        if self.downsample:\n",
    "            # downsample with stride =2 the input x\n",
    "            self.downsampleconv = SpatioTemporalConv(in_channels, out_channels, 1, stride=2)\n",
    "            self.downsamplebn = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "            # downsample with stride = 2when producing the residual\n",
    "            self.conv1 = SpatioTemporalConv(in_channels, out_channels, kernel_size, padding=padding, stride=2)\n",
    "        else:\n",
    "            self.conv1 = SpatioTemporalConv(in_channels, out_channels, kernel_size, padding=padding)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # standard conv->batchnorm->ReLU\n",
    "        self.conv2 = SpatioTemporalConv(out_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.outrelu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.relu1(self.bn1(self.conv1(x)))    \n",
    "        res = self.bn2(self.conv2(res))\n",
    "\n",
    "        if self.downsample:\n",
    "            x = self.downsamplebn(self.downsampleconv(x))\n",
    "\n",
    "        return self.outrelu(x + res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalResLayer(nn.Module):\n",
    "    r\"\"\"Forms a single layer of the ResNet network, with a number of repeating \n",
    "    blocks of same output size stacked on top of each other\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of channels in the input tensor.\n",
    "            out_channels (int): Number of channels in the output produced by the layer.\n",
    "            kernel_size (int or tuple): Size of the convolving kernels.\n",
    "            layer_size (int): Number of blocks to be stacked to form the layer\n",
    "            block_type (Module, optional): Type of block that is to be used to form the layer. Default: SpatioTemporalResBlock. \n",
    "            downsample (bool, optional): If ``True``, the first block in layer will implement downsampling. Default: ``False``\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, layer_size, block_type=SpatioTemporalResBlock, downsample=False):\n",
    "        \n",
    "        super(SpatioTemporalResLayer, self).__init__()\n",
    "\n",
    "        # implement the first block\n",
    "        self.block1 = block_type(in_channels, out_channels, kernel_size, downsample)\n",
    "\n",
    "        # prepare module list to hold all (layer_size - 1) blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for i in range(layer_size - 1):\n",
    "            # all these blocks are identical, and have downsample = False by default\n",
    "            self.blocks += [block_type(out_channels, out_channels, kernel_size)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R3DNet(nn.Module):\n",
    "    r\"\"\"Forms the overall ResNet feature extractor by initializng 5 layers, with the number of blocks in \n",
    "    each layer set by layer_sizes, and by performing a global average pool at the end producing a \n",
    "    512-dimensional vector for each element in the batch.\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes (tuple): An iterable containing the number of blocks in each layer\n",
    "            block_type (Module, optional): Type of block that is to be used to form the layers. Default: SpatioTemporalResBlock. \n",
    "        \"\"\"\n",
    "    def __init__(self, layer_sizes, block_type=SpatioTemporalResBlock):\n",
    "        super(R3DNet, self).__init__()\n",
    "\n",
    "        # first conv, with stride 1x2x2 and kernel size 3x7x7\n",
    "        self.conv1 = SpatioTemporalConv(3, 64, [3, 7, 7], stride=[1, 2, 2], padding=[1, 3, 3])\n",
    "        # output of conv2 is same size as of conv1, no downsampling needed. kernel_size 3x3x3\n",
    "        self.conv2 = SpatioTemporalResLayer(64, 64, 3, layer_sizes[0], block_type=block_type)\n",
    "        # each of the final three layers doubles num_channels, while performing downsampling \n",
    "        # inside the first block\n",
    "        self.conv3 = SpatioTemporalResLayer(64, 128, 3, layer_sizes[1], block_type=block_type, downsample=True)\n",
    "        self.conv4 = SpatioTemporalResLayer(128, 256, 3, layer_sizes[2], block_type=block_type, downsample=True)\n",
    "        self.conv5 = SpatioTemporalResLayer(256, 512, 3, layer_sizes[3], block_type=block_type, downsample=True)\n",
    "\n",
    "        # global average pooling of the output\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return x.view(-1, 512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R3DClassifier(nn.Module):\n",
    "    r\"\"\"Forms a complete ResNet classifier producing vectors of size num_classes, by initializng 5 layers, \n",
    "    with the number of blocks in each layer set by layer_sizes, and by performing a global average pool\n",
    "    at the end producing a 512-dimensional vector for each element in the batch, \n",
    "    and passing them through a Linear layer.\n",
    "        \n",
    "        Args:\n",
    "            num_classes(int): Number of classes in the data\n",
    "            layer_sizes (tuple): An iterable containing the number of blocks in each layer\n",
    "            block_type (Module, optional): Type of block that is to be used to form the layers. Default: SpatioTemporalResBlock. \n",
    "        \"\"\"\n",
    "    def __init__(self, num_classes, layer_sizes, block_type=SpatioTemporalResBlock):\n",
    "        super(R3DClassifier, self).__init__()\n",
    "\n",
    "        self.res2plus1d = R3DNet(layer_sizes, block_type)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.res2plus1d(x)\n",
    "        x = self.linear(x) \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crezione dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Propriet√† input e device:\n",
    "\n",
    "CLIP_LEN, RESIZE_HEIGHT, CROP_SIZE = 10, 128, 112\n",
    "resize_height = RESIZE_HEIGHT\n",
    "crop_size = CROP_SIZE\n",
    "clip_len = CLIP_LEN\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def center_crop(image):\n",
    "    height_index = math.floor((image.shape[0] - crop_size) / 2)\n",
    "    width_index = math.floor((image.shape[1] - crop_size) / 2)\n",
    "    image = image[height_index:height_index + crop_size, width_index:width_index + crop_size, :]\n",
    "    return np.array(image).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, directory, mode='train', clip_len=8, num_sec=8):\n",
    "        folder = Path(directory)\n",
    "\n",
    "        self.clip_len = clip_len-1\n",
    "        \n",
    "        self.num_sec=num_sec\n",
    "        \n",
    "        # the following three parameters are chosen as described in the paper section 4.1\n",
    "        self.resize_height = 128/2  \n",
    "        self.resize_width = 171/2\n",
    "        self.crop_size = 112/2\n",
    "\n",
    "        # obtain all the filenames of files inside all the class folders \n",
    "        # going through each class folder one at a time\n",
    "        self.fnames, labels = [], []\n",
    "        for label in sorted(os.listdir(folder)):\n",
    "            for fname in os.listdir(os.path.join(folder, label)):\n",
    "                self.fnames.append(os.path.join(folder, label, fname))\n",
    "                labels.append(label)\n",
    "                \n",
    "        # prepare a mapping between the label names (strings) and indices (ints)\n",
    "        self.label2index = {label:index for index, label in enumerate(sorted(set(labels)))} \n",
    "        \n",
    "        # convert the list of label names into an array of label indices\n",
    "        self.label_array1 = np.array([self.label2index[label] for label in labels], dtype=int)        \n",
    "        #create_dataset()\n",
    "        self.data_array = []\n",
    "        \n",
    "        self.label_array = []\n",
    "        for i in np.arange(len(self.fnames)):\n",
    "            fname = self.fnames[i]\n",
    "            files = [f for f in os.listdir(fname) if os.path.isfile(os.path.join(fname, f))]\n",
    "            clips = []\n",
    "            for f in files:\n",
    "                clip = cv2.imread(os.path.join(fname, f))\n",
    "                clips.append(clip)\n",
    "                inputs = np.array(clips)\n",
    "                inputs = np.expand_dims(inputs, axis=0)\n",
    "            \n",
    "            try:\n",
    "                buffer = np.transpose(inputs, (0, 4, 1, 2, 3))\n",
    "                buffer = buffer[0,:,:,:,:]\n",
    "            except:\n",
    "                buffer = np.transpose(inputs, (3, 0, 1, 2))\n",
    "                buffer = buffer\n",
    "            \n",
    "            if(np.shape(buffer)[1] != 1):\n",
    "                self.label_array.append(self.label_array1[i])\n",
    "                self.data_array.append(buffer)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return [self.data_array[index], self.label_array[index]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use GPU if available else revert to CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device being used:\", device)\n",
    "\n",
    "def train_model(num_classes, directory, layer_sizes=[2, 2, 2, 2], num_epochs=45, save=True, path=\"model_data.pth.tar\"):\n",
    "    \"\"\"Initalizes and the model for a fixed number of epochs, using dataloaders from the specified directory, \n",
    "    selected optimizer, scheduler, criterion, defualt otherwise. Features saving and restoration capabilities as well. \n",
    "    Adapted from the PyTorch tutorial found here: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "        Args:\n",
    "            num_classes (int): Number of classes in the data\n",
    "            directory (str): Directory where the data is to be loaded from\n",
    "            layer_sizes (list, optional): Number of blocks in each layer. Defaults to [2, 2, 2, 2], equivalent to ResNet18.\n",
    "            num_epochs (int, optional): Number of epochs to train for. Defaults to 45. \n",
    "            save (bool, optional): If true, the model will be saved to path. Defaults to True. \n",
    "            path (str, optional): The directory to load a model checkpoint from, and if save == True, save to. Defaults to \"model_data.pth.tar\".\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # initalize the ResNet 18 version of this model\n",
    "    model = R3DClassifier(num_classes=num_classes, layer_sizes=layer_sizes).to(device)\n",
    "    model.half()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() # standard crossentropy loss for classification\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)  # hyperparameters as given in paper sec 4.1\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # the scheduler divides the lr by 10 every 10 epochs\n",
    "\n",
    "    # prepare the dataloaders into a dict\n",
    "    train_dataloader = DataLoader(VideoDataset(directory), batch_size=6, shuffle=True, num_workers=4)\n",
    "    # IF training on Kinetics-600 and require exactly a million samples each epoch, \n",
    "    # import VideoDataset1M and uncomment the following\n",
    "    # train_dataloader = DataLoader(VideoDataset1M(directory), batch_size=32, num_workers=4)\n",
    "    val_dataloader = DataLoader(VideoDataset(directory, mode='val'), batch_size=4, num_workers=4)\n",
    "    dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n",
    "    \n",
    "    dataset_sizes = {x: len(dataloaders[x].dataset) for x in ['train', 'val']}\n",
    "    \n",
    "    # saves the time the process was started, to compute total time at the end\n",
    "    start = time.time()\n",
    "    epoch_resume = 0\n",
    "\n",
    "    # check if there was a previously saved checkpoint\n",
    "    if os.path.exists(path):\n",
    "        # loads the checkpoint\n",
    "        checkpoint = torch.load(path)\n",
    "        print(\"Reloading from previously saved checkpoint\")\n",
    "\n",
    "        # restores the model and optimizer state_dicts\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['opt_dict'])\n",
    "        \n",
    "        # obtains the epoch the training is to resume from\n",
    "        epoch_resume = checkpoint[\"epoch\"]\n",
    "\n",
    "    for epoch in tqdm(range(epoch_resume, num_epochs), unit=\"epochs\", initial=epoch_resume, total=num_epochs):\n",
    "        # each epoch has a training and validation step, in that order\n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            # reset the running loss and corrects\n",
    "            running_loss = 0.0;\n",
    "            running_corrects = 0;\n",
    "\n",
    "            # set model to train() or eval() mode depending on whether it is trained\n",
    "            # or being validated. Primarily affects layers such as BatchNorm or Dropout.\n",
    "            if phase == 'train':\n",
    "                # scheduler.step() is to be called once every epoch during training\n",
    "                optimizer.step() \n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                \n",
    "                # move inputs and labels to the device the training is taking place on\n",
    "                inputs = inputs.to(device);\n",
    "                labels = labels.to(device);\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # keep intermediate states iff backpropagation will be performed. If false, \n",
    "                # then all intermediate states will be thrown away during evaluation, to use\n",
    "                # the least amount of memory possible.\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    inputs = inputs.half()\n",
    "                    outputs = model(inputs);\n",
    "                    # we're interested in the indices on the max values, not the values themselves\n",
    "                    _, preds = torch.max(outputs, 1)  \n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backpropagate and optimize iff in training mode, else there's no intermediate\n",
    "                    # values to backpropagate with and will throw an error.\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()   \n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f\"{phase} Loss: {epoch_loss} Acc: {epoch_acc}\")\n",
    "\n",
    "    # save the model if save=True\n",
    "    if save:\n",
    "        torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'acc': epoch_acc,\n",
    "        'opt_dict': optimizer.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    # print the total time needed, HH:MM:SS format\n",
    "    time_elapsed = time.time() - start    \n",
    "    print(f\"Training complete in {time_elapsed//3600}h {(time_elapsed%3600)//60}m {time_elapsed %60}s\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch training and load weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#path = dataset path\n",
    "path = \"dataset_path\"\n",
    "\n",
    "#path_rete = where the weight of the network will be stored\n",
    "path_rete = \"NameNN.pth.tar\"\n",
    "\n",
    "net = train_model(4, path+str(fps), layer_sizes=[1, 1, 1, 1], num_epochs=10, save=True, path= path_rete)\n",
    "\n",
    "model = R3DClassifier(num_classes=4, layer_sizes=[1, 1, 1, 1]).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "x = torch.load(path_rete)\n",
    "\n",
    "model.load_state_dict(x['state_dict'])\n",
    "optimizer.load_state_dict(x['opt_dict'])\n",
    "acc= x['acc']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
